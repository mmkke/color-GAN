{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e723d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Nelson Farrell & Michael Massone\n",
    "Image Enhancement: Colorization - cGAN\n",
    "CS 7180 Advanced Perception\n",
    "Bruce Maxwell, PhD.\n",
    "09-28-2024\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8128a84b-045e-4bf2-87f7-a767c4735173",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from skimage.color import rgb2lab, lab2rgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cac4374e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/nelsonfarrell/Documents/Northeastern/7180/projects/color-GAN\n"
     ]
    }
   ],
   "source": [
    "path = Path(os.getcwd())\n",
    "path_to_project_home = path.parent\n",
    "path_to_project_home = str(path_to_project_home)\n",
    "print(path_to_project_home)\n",
    "sys.path.insert(1, path_to_project_home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "698710e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from src.utils.pretrain_utils import *\n",
    "from src.utils.gan_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7375ddf-d1fe-40bc-a6b0-84906cf1711e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d748e24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.learner import create_body\n",
    "from torchvision.models.resnet import resnet18\n",
    "from fastai.vision.models.unet import DynamicUnet\n",
    "\n",
    "def build_res_unet(n_input=1, n_output=2, size=256):\n",
    "    \"\"\"\n",
    "    Builds ResNet18 based U-Net\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    resnet_model = resnet18(pretrained=True)\n",
    "    body = create_body(resnet_model, pretrained=True, n_in=n_input, cut=-2)\n",
    "    net_G = DynamicUnet(body, n_output, (size, size)).to(device)\n",
    "    return net_G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "604298e0-86da-4746-aab7-7e3c9668d7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Preprocessing\n",
    "def preprocess_image(image_path):\n",
    "    # Load and resize the image\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    img = img.resize((256, 256))\n",
    "\n",
    "    # Convert to LAB color space\n",
    "    img_lab = rgb2lab(np.array(img)).astype(\"float32\")\n",
    "    \n",
    "    # Normalize L channel to range [-1, 1]\n",
    "    L = img_lab[..., 0:1] / 50.0 - 1.0\n",
    "    \n",
    "    # ab channels should be between [-1, 1]\n",
    "    ab = img_lab[..., 1:] / 110.0\n",
    "    \n",
    "    # Convert to tensors\n",
    "    L = torch.tensor(L).permute(2, 0, 1).unsqueeze(0)  # (1, 1, 256, 256)\n",
    "    ab = torch.tensor(ab).permute(2, 0, 1).unsqueeze(0)  # (1, 2, 256, 256)\n",
    "    \n",
    "    return L, ab, img_lab\n",
    "\n",
    "# Inference on L channel\n",
    "def run_inference_on_L(model, L):\n",
    "    with torch.no_grad():\n",
    "        ab_pred = model(L)\n",
    "    return ab_pred\n",
    "\n",
    "# Recompile the LAB image and convert back to RGB\n",
    "def reassemble_and_convert_to_rgb(L, ab_pred):\n",
    "    # Denormalize L and ab channels\n",
    "    L = (L.squeeze(0).squeeze(0).cpu().numpy() + 1.0) * 50.0  # back to [0, 100] range\n",
    "    ab_pred = ab_pred.squeeze(0).cpu().numpy() * 110.0  # back to [-110, 110] range\n",
    "    \n",
    "    # Reassemble LAB image\n",
    "    lab_pred = np.concatenate([L[..., np.newaxis], ab_pred.transpose(1, 2, 0)], axis=-1)\n",
    "    \n",
    "    # Convert LAB to RGB\n",
    "    rgb_pred = lab2rgb(lab_pred)\n",
    "    return rgb_pred\n",
    "\n",
    "# Visualize the images\n",
    "def visualize_images(original_img, reconstructed_img):\n",
    "    \"\"\"\n",
    "    Displays\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    ax[0].imshow(original_img)\n",
    "    ax[0].set_title(\"Original Image\")\n",
    "    ax[1].imshow(reconstructed_img)\n",
    "    ax[1].set_title(\"Predicted Image\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Visualize lists of images\n",
    "def visualize_images_2(original_imgs:list, reconstructed_imgs:list, inputs:list, save_path:str) -> None:\n",
    "    \"\"\"\n",
    "    Saves a figure of a set of example images: orignal, inputs, and generated.\n",
    "    Adjust fig_size length as needed for the length of the list.\n",
    "\n",
    "    Args:\n",
    "     * original_images: (list)\n",
    "     * reconstructed_images: (list)\n",
    "     * inputs: (list)\n",
    "     * save_path: (str)\n",
    "\n",
    "     Returns: \n",
    "      * None\n",
    "    \"\"\"\n",
    "    n = len(original_imgs)  # Number of images\n",
    "    fig, axs = plt.subplots(n, 3, figsize=(10, 10))  # Create a grid of subplots\n",
    "    \n",
    "    for i in range(n):\n",
    "        input_img = np.squeeze(inputs[i]) if inputs[i].shape[0] == 1 else inputs[i]\n",
    "\n",
    "        # Display original images\n",
    "        axs[i, 0].imshow(original_imgs[i])\n",
    "        if i == 0:\n",
    "            axs[i, 0].set_title(f\"Original Image\", weight = \"bold\")\n",
    "        axs[i, 0].axis('off')\n",
    "\n",
    "        # Display reconstructed images\n",
    "        axs[i, 2].imshow(reconstructed_imgs[i])\n",
    "        if i == 0:\n",
    "            axs[i, 2].set_title(f\"Generated Image\", weight = \"bold\")\n",
    "        axs[i, 2].axis('off')\n",
    "\n",
    "        # Display input images\n",
    "        axs[i, 1].imshow(input_img, cmap='gray')\n",
    "        if i == 0:\n",
    "            axs[i, 1].set_title(f\"Input Image\", weight = \"bold\")\n",
    "        axs[i, 1].axis('off')\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.1) \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc5620c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nelsonfarrell/miniconda3/envs/GAN_env_CUDA11_8/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/nelsonfarrell/miniconda3/envs/GAN_env_CUDA11_8/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = build_res_unet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0aace90d-1d53-4453-939a-c037bf1f2f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained model\n",
    "model = build_res_unet()\n",
    "path_to_weights = \"/Users/nelsonfarrell/Documents/Northeastern/7180/projects/color-GAN/model_checkpoints/checkpoint_pretrain_gen.pth\"\n",
    "checkpoint = torch.load(path_to_weights, map_location=torch.device('cpu'))\n",
    "checkpoint = checkpoint[\"generator_state_dict\"]\n",
    "model.load_state_dict(checkpoint)\n",
    "model.eval()  # Set the model to inference mode\n",
    "\n",
    "# Load data\n",
    "coco_path = \"/Users/nelsonfarrell/.fastai/data/coco_sample/train_sample\"\n",
    "paths = glob.glob(coco_path + \"/*.jpg\") \n",
    "\n",
    "# Get val data\n",
    "num_imgs = 10000\n",
    "split = 0.8\n",
    "train_paths, val_paths = select_images(paths, num_imgs, split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "461abe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_results_list = [   \n",
    "                    \"/Users/nelsonfarrell/.fastai/data/coco_sample/train_sample/000000319579.jpg\",\n",
    "                    \"/Users/nelsonfarrell/.fastai/data/coco_sample/train_sample/000000100271.jpg\",\n",
    "                    \"/Users/nelsonfarrell/.fastai/data/coco_sample/train_sample/000000107846.jpg\",\n",
    "                    \"/Users/nelsonfarrell/.fastai/data/coco_sample/train_sample/000000064121.jpg\",\n",
    "                    \"/Users/nelsonfarrell/.fastai/data/coco_sample/train_sample/000000547471.jpg\",\n",
    "                    \"/Users/nelsonfarrell/.fastai/data/coco_sample/train_sample/000000411138.jpg\"\n",
    "                    ]\n",
    "\n",
    "bad_results = [\n",
    "                \"/Users/nelsonfarrell/.fastai/data/coco_sample/train_sample/000000092602.jpg\",\n",
    "                \"/Users/nelsonfarrell/.fastai/data/coco_sample/train_sample/000000450649.jpg\",\n",
    "                \"/Users/nelsonfarrell/.fastai/data/coco_sample/train_sample/000000367853.jpg\"\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c08362e-6fe4-4fdd-bfe3-864559949e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/nelsonfarrell/.fastai/data/coco_sample/train_sample/000000092602.jpg\n",
      "/Users/nelsonfarrell/.fastai/data/coco_sample/train_sample/000000450649.jpg\n",
      "/Users/nelsonfarrell/.fastai/data/coco_sample/train_sample/000000367853.jpg\n"
     ]
    }
   ],
   "source": [
    "# Run the pipeline\n",
    "original_image_list = []\n",
    "reconstructed_img_list = []\n",
    "grey_image_list = []\n",
    "save_path = \"figs/resGAN_bad.png\"\n",
    "for image_path in bad_results:\n",
    "\n",
    "    # Preprocess the image\n",
    "    L, ab, original_lab = preprocess_image(image_path)\n",
    "    \n",
    "    # Run inference\n",
    "    ab_pred = run_inference_on_L(model, L)\n",
    "    \n",
    "    # Reassemble and convert to RGB\n",
    "    reconstructed_img = reassemble_and_convert_to_rgb(L, ab_pred)\n",
    "    \n",
    "    # Convert original LAB back to RGB for comparison\n",
    "    original_rgb = lab2rgb(original_lab)\n",
    "\n",
    "    original_image_list.append(original_rgb)\n",
    "    reconstructed_img_list.append(reconstructed_img)\n",
    "    grey_image_list.append(L)\n",
    "    \n",
    "    # Visualize original and reconstructed images\n",
    "    print(image_path)\n",
    "    #visualize_images(original_rgb, reconstructed_img)\n",
    "\n",
    "\n",
    "visualize_images_2(original_image_list, reconstructed_img_list, grey_image_list, save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GAN_env_CUDA11_8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19 (main, May  6 2024, 14:39:30) \n[Clang 14.0.6 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "975f4b5d9e0919a43278ea8b2b4192a66d11eefdd80f30b2fc38044fc7c9a152"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
